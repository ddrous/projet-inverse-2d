% Chapter 4

\chapter{Apprentissage} % 4th chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter4} 

L'objectif de cette section est de reconstruire la densite en connaissant l'energie E, le flux F,et la temperature sur les bords du domaine aux cours du temps. Dans la suite, nous ferons une simplification majeure: la densite est supposee un signal en creanu (la forme du crenau vu du haut pouvant etre non connue). Aisni, reconstruire la densite revien juste a predire la position et la hauteur du crenau. La valeur de la densite en dehors du crenau sera aussi supposee connue. Nous recherchons une fonction $f^{-1}$ invese de $f$(fonction definisssant le probleme direct) telle que $y = f^{-1}(X)$. OU y represente la densite(plus precisement les attribut de son aut de densite), et X la les signaux sur les bords. Mais le caractere naturellement mal pose des problem inverse rend difficile la determination de $f^-1$. On procede donc a une approximation par un reseau de neurnoes artificiel (ANN) de $f^{-1}$ notee $$y = HATf^{-1}(X, \theta)$$, Ou $\theta$ represente les parametres du reseau de neuronnes.

%----------------------------------------------------------------------------------------

\section{Description des entrees/sorties}

\subsection{Les entrees}
Les entree sont onpossee des signaux etmporels E, F, et T. A chaque fois, il faut normaliser avant de les nourir au reseau de neuronnes. 

\subsubsection{en 1D}
En 1D, les entrees ne sont constituees que di signal recuperer sur le bord droit du domaine. La forme d'une example en presentee a la figure ... .
\subsubsection{en 2D}
Une entree 2D contient considerableme plus d'informatiosn. Les 3 signaux E, F et T sur les 4 bords y sont inclus. On y inclu aussi un signal correpondant a l'une des quatre positions de la source dans chacun de ces cas. En gros, une source a la forme suivante:

Des exampesl d'entree ont ete sauvegardee et les details pour els recuperer sont donnes en Anexe.

\subsection{Les sorties}
Comme mentione plus hautm nous avons faitr quelques simplifications sur la nature des sorties.
\subsubsection{en 1D}
Il s'agit uniquement de l'abcisse et de la hauteur du saut de densite.
\subsubsection{en 2D}
Comparer a la 1D, il faut rajouter l'ordonnee du saut de densite.

%----------------------------------------------------------------------------------------

\section{Architecture generale}

Un reseau de neuronnes artificel \footnote{nous y fereons reference dans la suite juste par reseau de neurones} est un systeme computationnel base sur le reseau de neuronnes biologique. L'apprentissage profond\footnote{definition du nombre de couche} permet de resoudre des problemes en Machine Learning\footnote{definition} que les methodes telles que la regression ineaire, etc.. ne peuvent pas. Il reussit cela en introduisant des representations des donnes qui s'exprimes sous forme d'autres representations, plus simples cette fois. Les reseaux profonds en aval\footnote{en oposition a un reseau de neurones recurrent qui reutilisent els resultats des model pour s'ameliorer} (ou MLP) consitituent l'exemples typique en apprentissage profond. Il s'agit juste d'une fonction (composition de differentes fonctions) faisant correspondre une serie d'entree a une serie de sortie $f^{-1} = composition de f1, f2, etc.$.

Un MLP est constitue de plusieurs couche (assimilables aux fonction f1, f2, .. precedentes) apprenant chacune un aspect particulier des donnnees.
\begin{itemize}
 \item une couche d'entree
 \item des couches cachee
 \item une couche de sortie
\end{itemize}


(IMAGE D'UN MLP)

Les reseaux de neurones convolutifs sont une forme de MLP spcialises dans le traitement des donnes qui ont une form de grille. Par exemple des series en temps qui peuvent etres vues comme des grilles 1D prenant des samples a interval de temps regulier \parencite{Reference5}. Ils sont donc particulieremt adaptes a la reconstruction de la densite partant des signaux temporels E, F, et T. L'archiytecture de base (proposee par M. Vigon ) que nous allons utiliser est representee a la figure .... L'architecture sera implemntee sous la livrarie de machine leanring Keras (avec Tensorflow backend) Les differentes couches presentes seront detailles dans la suite. Nous indiquerons aussi en quoi elles sont importantes pour notre apprentissage.

(ARCHITECTURES DE BASE, avec et sans pooling)

%----------------------------------------------------------------------------------------

\section{Les couches utilis√©es}

\subsection{Les couches de convolution}
la convolution est l'operation fondamentale d'un CNN. Il s'agit d'une operation lineaire qui combine deux signaux pour en extraire un troisieme. En general, une operation de convolution se definit par la formule suivante.
(FORMULE DE CONV 9.1 - s(t) = i*k) (ici i est le ssignal d'entree et k est le noyau de la convolution)

En pratique, les signaux temporels ne sont pas continus, ils sont discretises par interval de temps $\Delta t$. Dans ce contexte, la convolution 1D se definit par la formule:
(FORMULE 9.3)
Cette formule doit aussi etre adaptee en 2D vu que nos inputs sont 2D. La formule devient donc:
(FORMULE 9.4)

L'operation de convolution est commutative grace a l'inversion du noyaux relativement au siganal d'entree. Cette propriete, bien qu'importante d'un point de vu therique, ne prensente pas d'avantages majeure du point de vu computationnel. C'est la raison pour laquelle on dispose de l'operation de cross-correlation qui est convolution sans inversion du noyau.

(CROSS-CORRELAYION 1D et 2D - 9.6)

On remarque aussi que le parcours des indices se fait suivant l'input. Il se trouve que c'est plus direct et rapide ainsi, parcequ'il y a moins de varaition dans la plage de valeurs valides pour n et m. 

Plueieurs libraries de machine leanring implemente implementent la cross-corelation mais l'appellent convolution. C'est le cas de Keras lorsqu'elle utilise le backend Tensorflow \parencite{Reference5}.
(IMAGE D'UNE CONV 1D - en mode valide)
(IMAGE D'UNE CONV 2D)

Sous Keras, les proprietes du couche de convolution sont:
(PROPREITES 1D)
(PROPREITES 2D)


Les CNN apportent trois notions cles a un apprentissage:
\begin{itemize}
 \item l'interaction creuse: contrairemetn aux couches traditionneles, les couches de convolution utilisent des noyaux de taille condiereblemen inferieure a celle de l'input. En terme de multiplication matricielle, cela permet de faire des taches toutes aussi importantes (detection des condouts, floutage, etc..) en ne gardant que peu de parametres en memoire et augmentant l'efficacite statitique. Cela permet aussi de reduire les couts de calcul.
 (IMAGE)
 \item le partage des parametres: les coefficient du noyau de convolution sont reutilises a chaque endroit de la matrice d'entree, contrairement aux couches traditionnelles qui utilise generalement chaque coefficient une seule fois.
 (IMAGE)
 \item la representation equivariante: le partage de parametre introduit la proprite d'equivaraition par translation. SI l'entree change, la sortie change de la meme facon, et le reseau de neurones exploite cela. Par exemple, dans l'etude d'une image, il serait interressant de detecter les contour dans la premiere couche du reseau,vu que ces meme contour sont suceptibles de reaparatire dans la suite. (\parencite{Reference5})
\end{itemize}


Les CNN se prensentent comme un example de principes neuroscientifiques appliques a l'aprentissage machine (NEURSICNCE).En pratigque, presque tous les reseaux de neuronnes convolutifs utilisent une operation appelee "pooling". En effet, dans les architecture de CNN typiques, la couche de convolution est generalement suivi d'une etape dite de detection. Dans cette etape, les resultas lineaires de la convolution sont passes a une fonction non lineaire auniveau d'une couche de d'activaiotn. Nous detaillerons les details de l'activation dans les sections suivantes. Apres cette etape de detection, le pooling est applique pour modifier les resulats encore plus profoncdement.

\subsection{Le MaxPoling}
Une fonction de pooling tranforme les entres voisines par une fonction d'aggregation statistique. Plusieurs fonctions d'aggregations peuvent etres utlisees. Par examples, le maxpooling renvoi le maximum parmis les entrees sur un domaine (rectigne en 1D et rectangulaire en 2D). 

(IMAGE DE MAX POOLING 1D et 2D)

En general, l'operation de pooling permet de rendre la representation approximativemtn invariante aux petite variatons dans l'input. Parlant de l'identifcation d'objects dans une image par exemple, \textit{l'invarianve par translations locales (petites tranalations) peut etre utile si on est plus interresse par la presence de l'object que par sa localisation exacte}\parencite[321ff.]{Reference5}.

Dans le probleme inverse que nous resolvons, on est aimerais non seulement detecter la presence du saut de densite, mais aussi ses coordonnes exactes. Cela nous amenera donc a considerer dans un premier temps une architecture sans pooling, et dans un dexieme temps, avec Pooling.
 
\subsection{Flatten}
L'operation d;applattissage permet de transformer les donnees en quittant de la forme tensorielle (2D avec plusieurs canaux) a une forme vectorielle. Il s'agit en realite d'une etape de preparation a une couche complement connectee.

(IMAGE D'UN FLATTEN)

\subsection{Les couches denses}
Dans cette couche, tous les neurones sont connectes a tous les neurones de la couche precedente. Une couche dense prend les resultats d'une convolution/pooling et en resort des poinds. Les couches de convolution ayant apris des aspects particuliers des donnees, la couche est un moyen facile d'apprndre des combinaisons non lineaires de ces dernieres.

(IMAGE D``UNE COUHE DENSE)

%----------------------------------------------------------------------------------------

\section{Configurations de l'entrainement sous Keras}
Keras proposent une multitude d'otions et d'hyperparatres pour tuner le modele. Les plus importants sont detailles dans les sections suivantes.

\subsection{Les hyper-parametres}

\subsubsection{le taux d'apprentissage}
il s'agit du parametre le plus influant pour notre apprentissage. Il controle a quelle vitesse le modele \footnote{les poids des neurones sont initialises de facon aleatoire} s'adapte au probleme en determinant de quelle quantite les poids des neurones seront mis a jour apres l'agorithme de backpropagation. S'il est tres eleve, il raoidement conduire a solution non optimale; s'il est tres faible, le modele peut reste fige (il fadra alors un nombre eleve d'epoques pour le debloquer).

Avec un taux d'apprentissage egale a 1e-3, nous n'avons ete capable que de detecter la hauteur du crenau en 1D. Cependant il a fallu descendre jusqua 1e-5 pour determiner avec precision l'abcisse, l'ordonne, et la hauteur du crenau en 2D.

\subsubsection{le batch size}
Il s'agit de la taille de chauque paquet de donnnes \footnote{nombre d'instaces d;entrainement selectiones aleatoiremetn} passes au modele durant un epoque. Un batch size faible apporte du bruit au modele vu qu'une partie aleatoire des donnes est tulisee pour mettre ajour les poids des neurones. Ceci permet une meilleure generalisation du modele tout en permettant une limiter la quantite de donnees chargee dans la RAM a chaque epoque.

\subsubsection{le coefficient de reglarisation L2}
Le penalisation permet d;eviter le surapprentissage. (FORMULE DE PENALISATION) Sous Keras, on peut soit penaliser les poids d'une couche (kernel optimizer), ou bien penaliser les resultats de la fonction d'activation de cette couche (activity optimizer). la deuxieme option a offert les meilleures resultats, c'est pourquoi l'avons appliquee aux deux couches de neuronnes denses avec un coefficient de 1e-5.

\subsection{Autres options utilises}

\subsubsection{L'optimiseur}
L'optimization est une methode d'acceleration de l'entrainement. L'optimiseur Adam\footnote{Adaptative moment estimation} combine les proprietes de deux autres algorithmes d'entrainement (AdaGrad et RMSProp).

\subsubsection{Activation RELU}
La fonction d'activation introduit une non-linearite entre les couche. L'avatage majeure de l'activation ReLU\footnote{Rectified Linear Unit} par rapport aux autres fonctions d;activation c'est qu'il n'active pas tous les neurones en meme temps. D'un pooint de vue computationel, elle est tres eficace tout en produidant des resulats satisfaisants.

\subsubsection{Early stopping}

La technique d'early sera notre moyen primaire de lutte contre le sur-appretisage. Nous arreterons l'apprentissage lorsque le score de validation n'aura pas augmente sur 10 epoques.

\subsubsection{Loss MSE}
Pendant la generation des donnees, on a pris soin de pas introduire de donnes aberantes. La MSE qui est plus elevee sur les valeurs aberantes que la MAE est dont plus adaptee ici.
(FORMULE DE MSE)

\subsubsection{Coefficient de determination R2}
Il se definit comme etant le caree du coefficient de correlation entre les predictions et les labels. Alternativement, on peut utiliser la formule suivante:

(FORMULE POUR R2)

On voit que les predictions et les labels sont tres correles sant etre egaux (sans etre egaux), on risque d'avoir un score R2 ce qui n'est pas carateritique des resultats.

\subsubsection{Un score personalise}
On definit donc un nouveau score particulieremtn adapte a nos donnees. On suppose la prediction correcte si elle est suffisament proche du label:
\begin{itemize}
 \item au dizi√®me pr√®s pour la position (suivant x ou y)
 \item √† l'unit√© pr√®s pour la hauteur
\end{itemize}

%----------------------------------------------------------------------------------------

\section{Resultats}

Nous resumons la sections precedentes en specifiant les paramtres (et leurs noms) utilises tel que mentiones sous Keras.

(TABLEAU 1D 2D de tous les parametres)

Nous avons entraine les architectures en 1D (fig ...) et par la suite en 2D (fig ...) en ajustant les dimensions des coushes connablement.

\subsection{R√©gression}
% 
    \subsubsection{en 1D}
    On obtient un score R2 de l'ordre de 98\%. On obtient de tres bonnes predictions sur la hateur de l'obstacle qui affecte directement l'amplitude des signaux sur le bord droit du domaine. Ce score n'est pas assez indicatif vu que les predictions sur la position du crenaux ne sont pas assez precises. En effet, le score personalise ne vaut que 26\%.
    
    Ci-dessous sont quelques unes des pires predictions du modele.
    
    (AFFICHER LES PIRES PREDICTIONS 1D)

    Pour remedier a ce probleme de detection de position x d'un obstacle, il faut passer en 2D.  En effet, le probleme inverse est naturellement mal defini dans le sens ou plusieurs entree peuent donner la meme sortie. En 1D, on ne peut mesurer la sortie que sur un seul bord du domaine, ce qui limite beacoup notre aaprntissage.
    
    \subsubsection{en 2D}
    On obtient un score R2 de l'ordre de 98\%. Cette fois le reseau est capable de detecter non seulment la hauteur de l'obstacle, mais aussi son abcisse et son ordonnee. Notre score personnalise nous permet de confirmer cela avec un score (severe) d'environ 92\%.
    
    Ci-dessous sont quelques unes des pires predictions du modele.
    
    (AFFICHER LES PIRES PREDICTIONS 2D)
    
    Le modele se generalise tres bien. En l'utilisant pour predire des obstacles de nature differente, (des rectangles vu du haut au lieu des cercles), le modele s'ensort plutot bien avec des score de .....
    En plus, le modele a prouver etre capable d'apprendre en continu, du moment que les entrees soient toutes normalisee et ayant la meme forme.

\subsection{Classification}
Durant le stage, il a fallu effectuer une classification mutilabel sur les donnes en 2D. QUi permet de placer l'obstacle dans une categorie definie a partir de la source. La classification petmet de detecter juste l;ordonne de l'obstacle. L'image ci-dessous decrit mieux cette classification:

(IMAGE DEFINITION DE LA CLASSIF)

Les donnnes utilisee pour la classification ont une shape differente des autre. On a moins d'iterations en temps mais mais un maille beacoup plus fin (90x90).

Le modele performe relativement bien sur ces taches de classification avec un score que j'ai appele multilabel accuracy (hard) de .. \%. 
Quelques undes de pires prediction sont les suivantes.

%----------------------------------------------------------------------------------------
